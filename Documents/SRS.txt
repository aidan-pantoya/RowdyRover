AI Navigation:
    "Lane assist":
        Implement an algorithm that uses on-board cameras to detect rows of crops and stay within an acceptable distance around the currently traversed row.
    "Turning":
        Implement an algorithm that detects when rover is no longer over a row, will then find next row to traverse without going out-of-bounds of intended traversal area, skipping rows, and re-traversing rows
    "Object detection":
        Implement an algorithm that detects unusual objects in navigation path to avoid or stop for. Can stop for people and large objects and can try to avoid small objects if possible.
    "Navigation area":
        Be able to define an area for the rover to navigate. This can be done through detecting rows to traverse or through defining an area to traverse (GPS coordinates to define the shape of the field? Not desirable but is an option)

Photogrammetry (Image stitching):
    "Map Append":
        Implement photogrammetry algorithms that will take live feed from the cameras to stitch together a map of each row onto an dynamic-size map or fixed-size map
    "Map":
        Either predefine space to insert rows onto (drone image, GPS coordinates, etc) or create map dynamically as rover scans rows.

VR (Optional):
    "Live VR Environment":
        I don't think this is feasible to be honest. The bandwidth needed for a stable connection wirelessly would require at least 5GHz for a home internet setup. Cellular speeds slower than this (I don't know which cellular types equate to that) would lead to a suboptimal experience and a waste of effort (unless we could buffer it somehow, which would not make it "live")
    "VR Environment":
        We could somehow upload each map the rover creates after each scan to a website or something for the user to view in a VR headset browser. Idk how to implement this or where to begin.
    
AI Worker:
    Implement something to detect if a certain area of crop needs pesticides, water, fertilizer, etc. Not sure if we can fit this into our current objectives.


    Example document: (ChatGPT)

Software Requirements Specification (SRS) for Crop Mapping Rover
    Project Overview
        The Crop Mapping Rover aims to autonomously navigate through a field, capturing synchronized images of crop rows with three cameras and processing the data to create a 3D map of the entire field. 
        This SRS document outlines the software and algorithm development, as well as the post-processing and analysis requirements for this project.
    Software and Algorithm Development
        Navigation Software
            The navigation software shall provide autonomous movement capabilities to the rover.
            It shall include obstacle avoidance algorithms to prevent collisions with field obstacles.
            The software shall support predefined path-following and autonomous row detection modes.
        Camera Control Software
            The camera control software shall synchronize the three cameras for simultaneous image capture.
            It shall allow control over camera settings such as exposure, focus, and capture rate.
            The software shall trigger image capture based on predefined criteria (e.g., distance traveled, GPS coordinates).
            Data Synchronization
            Data from the cameras, IMU, and GPS shall be synchronized in a common timestamped format.
            The synchronization accuracy shall be within Â±5 milliseconds to ensure accurate data fusion.
        Image Processing Algorithms
            Image processing algorithms shall be implemented for feature detection, image stitching, and crop row tracking.
            The software shall use computer vision techniques to identify and track crop rows in real-time.
        3D Reconstruction
            Photogrammetry algorithms shall be employed for 3D reconstruction from synchronized images.
            The software shall generate point clouds and/or dense 3D meshes of crop rows.
        Data Storage
            Captured images, sensor data, and 3D reconstruction results shall be organized and stored in a structured database.
            Data storage shall support efficient retrieval and archiving.
    Post-Processing and Analysis
        3D Mapping Software
            The 3D mapping software shall combine individual crop row reconstructions to create a cohesive 3D map of the field.
            It shall employ alignment and merging techniques to ensure accuracy and completeness.
        Data Visualization
            The software shall provide tools for visualizing the 3D map and associated crop data.
            Visualization options shall include 3D renderings, heatmaps, and graphical representations.
        Data Interpretation
            Algorithms shall be developed to analyze crop data within the 3D map.
            The software shall provide insights into crop health, density, and any anomalies detected.
        Data Storage and Sharing
            Final 3D map data and analysis results shall be stored in a secure and accessible format.
            The software shall allow for data export and sharing with relevant stakeholders.
    Non-Functional Requirements
        The entire system shall operate reliably in varying weather and lighting conditions.
        Response times for real-time processing shall not exceed 100 milliseconds.
        The system shall be designed to minimize power consumption for extended field operations.
        Data security measures shall be implemented to protect sensitive agricultural information.
    Conclusion
        This SRS document outlines the high-level software and algorithm development and post-processing and analysis requirements for the Crop Mapping Rover project. 
        Detailed technical specifications, diagrams, and user interface designs will be developed in subsequent project phases.


Example Document: (Me)

Rowdy Rover SRS
    Introduction
        This project will be developed for the purpose of aiding farmers observe their crops as they grow. This will help them detect anomolies in the growing process and be able to generally observe the quality of the crops more conviently.
        The project will take place over a span of two university semesters at WTAMU. The first semester will focus on the navigation of the rover and the second semester will focus on the development of creating the 3D map of the field and the rows of crop.
    System Architecture
        As it stands, the rover will utilize a light-weight chassis equiped with three high resolution cameras and a Raspberry Pi used for navigation and image processing.
        A 3D camera can can be attached to ease the computational load for the Raspberry Pi for the potential VR use case.
        If necessary, additional hardware can be equiped to ensure fulfilment of requirements and optimal performance.
    Hardware Requirements
        We will be using three Intel Real Sense D415 depth cameras to scan the rows of crops. A Raspberry Pi 4B will interface with the onboard hardware to control the rover, utilize information from onboard sensors, and provide the computational power required for navigation and image processing.
        (Need to put more here for additional hardware specifications)
    Functional Requirements
        Navigation
            We will utilize Simultaneous Localization and Mapping (SLAM) algorithms to determine the robot's location within the map of the field being created.
            Two such algorithms include the EKF-SLAM algorithm and Graph-Based SLAM methods.
            Implement an algorithm that detects when rover is no longer over a row, will then find next row to traverse without going out-of-bounds of intended traversal area, skipping rows, and re-traversing rows.
            Implement obstacle avoidance using techniques such as Potential Fields and Reactive Navigation.
        Image Stitching and Photogrammetry
            We will implement feature detection and matching algorithms such as SIFT, SURF, and ORB which will detect distinctive local features in images for use in stitching images together.
            Some image stitching algorithms we could implement would include RANSAC and Homography Transformation.
            To construct the 3D image, we will utilize Structure-from-Motion and Bundle Adjustement.
            For dense 3D meshes, we will utilize Multi-View Stereo techniques.
            Depth will be calculated using the attached depth sensing cameras or if that option becomes unavailable, we could implement Stereo Vision to estimate depth from disparity between images.
            We will visualize the map using common 3D model software and/or OpenGL or PCL.
    Nonfunctional Requirements
        The rover will need to be able to detect obstacles in real time. The rover will need to be lightweight and travel in a way such that it will not collide with any crops or get stuck in the mud.
        Energy consumption will need to be within the capabilites of the attached solar panel and must be optimized so that the rover will not die while on the field.
        The rover must be able to work autonomously on the field without any human interaction or intervention to operate as expected.
        There will be an interface to interact with the rover in real time to observe its operational status, observe environmental variables, and detect and potential anomolies or errors in the operation of the rover.
        The rover will work during specified operation times of the day and will operate each specified day of each week.